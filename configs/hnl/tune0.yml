# HNL tune0.yml
#
# -------------------------------------------------------------------

rootname: 'hnl'
rngseed: 123456                       # Fixed seed for training data mixing
num_cpus: 0                           # 0 for automatic
inputvars: 'mvavars'                  # Main file input description python file

# ----------------------------------------------------
mva_param: &MVA_INPUT_PARAM
  use_conditional: false              # Conditional (theory parametric) input
  primary_classes: [0,1]              # Primary class IDs in MVA (train, ROC etc.)
  signal_class: 1                     # Signal class ID
  #DA_class:    -2                    # Domain Adaptation class
  
  inputvar_scalar:  'MVA_SCALAR_VARS' # 'MVA_SCALAR_VARS' # Input variables, implemented under mvavars.py
  #inputvar_jagged:  null             # 'MVA_JAGGED_VARS'
  #jagged_maxdim:    6
  
  varnorm: null                       # Variable normalization: 'zscore', 'zscore-weighted', 'madscore', null
  #varnorm_tensor: 'zscore'           # Tensor variable normalization
  #varnorm_graph: null                # Not implemented yet
  
  frac: [0.6, 0.1, 0.3]               # Train vs validate/test split fraction
  
  # Imputation
  imputation_param:
    active: true                      # True / False
    var: 'MVA_SCALAR_VARS'            # Array of variables to be imputated
    algorithm: 'constant'             # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
    fill_value: -99                   # For constant imputation
    knn_k: 8                          # Number of nearest neighbours considered
    values: null                      # Special values which indicate the need for imputation, if null, then only Inf/Nan
  
  # # Graph object construction
  # graph_param:
  #   global_on: True
  #   self_loops: True
  #   directed: False
  #   coord: 'pxpypze'                 # 'ptetaphim', 'pxpypze'

  # # ** Image tensor object construction **
  # image_param:

  #   # See the corresponding construction under common.py
  #   channels: 2                 # 1,2,...

  #   # bin-edges
  #   eta_bins: []
  #   phi_bins: []


# ----------------------------------------------------
genesis_runmode:

  maxevents:  null
  inputmap:   null #"mc_input.yml"
  
  mcfile:     ['bdt_df_2016.parquet']
  datafile:   null
  tree_name:  null                    # 'ntuplizer/tree'
  
  targetfunc: null                    # Training target,    implemented under mctargets.py
  filterfunc: 'filter_standard'       # Training filtering, implemented under mcfilter.py
  cutfunc:    'cut_fiducial'          # Basic cuts,         implemented under cuts.py

  xcorr_flow: True                    # Full N-point correlations computed between cuts
  pickle_size: 100000                 # Number of entries (events) per pickle file


# ----------------------------------------------------
train_runmode:

  <<: *MVA_INPUT_PARAM

  maxevents: null
  modeltag:  null

  tech: &TECH
    concat_max_pickle: 32           # (technical) [Recursive] concatenation max size, adjust this if encounter problems e.g. with awkward ("127 limit")
  
  ## Reweighting setup
  reweight: true
  reweight_mode: 'write'          # 'write', 'load'
  reweight_file: null                 # null for automatic, or string for specific
  
  reweight_param: &REWEIGHT_PARAM

    equal_frac: true              # Equalize integrated class fractions
    differential: false           # Differential reweighting
    reference_class: 0

    # ---------------------
    # Differential param
    # For example, see /trg/tune0.yml


  ## Outlier protection in the training
  outlier_param:
    algo: 'truncate'   # algorithm: 'truncate', null
    qmin: 0.01         # in [0,100] 
    qmax: 99.9         # in [0,100]

    truncate_weights: True # Truncate outlier event weights
    process_validate: True # Protect also validation sample
  
  # ** Activate models here **
  # Give all models some unique identifier and label
  models:  !include configs/hnl/models.yml
  active_models: &ACTIVE_MODELS
    
    #- xgb-MI-max
    #- dmlp-MI-max

    - xgb-MI-min
    - dmlp-MI-min
    
    - xgb0
    - dmlp0
    
    - xgb0-red
    - dmlp0-red
    
    - cut0
    - cut1
  
  raytune: !include configs/hnl/raytune.yml

  # Distillation training
  # -- the order must be compatible with the causal order in 'active_models'
  distillation:

    # Big, sophisticated model
    source:
      #xgb0
    
    # Simple, compressed models
    drains:
      #- xgb1
      # - add more here

  # Batched "deep" training
  batch_train_param:
    blocksize: 150000   # Maximum number of events simultaneously in RAM
    epochs: 50          # Number of global epochs (1 epoch = one iteration over the full input dataset), same for all models
    #num_cpu: null      # Set null for auto, or an integer for manual.

# ----------------------------------------------------
eval_runmode:

  <<: *MVA_INPUT_PARAM

  maxevents: null
  modeltag:  null
  
  tech: *TECH
  
  reweight: true
  reweight_mode: 'load'          # 'write', 'load'
  reweight_file: null                 # null for automatic, or string for specific
  
  reweight_param: *REWEIGHT_PARAM

  models:  !include configs/hnl/models.yml
  active_models: *ACTIVE_MODELS

# ----------------------------------------------------
plot_param: !include configs/hnl/plots.yml

