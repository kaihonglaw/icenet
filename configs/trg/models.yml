## MVA models

cutset0:
  train:   'cutset'
  predict: 'cutset'
  
  label:   'cutset'
  raytune:  cutset_setup_0
  
  # Optimization
  opt_param:  
    
    #lossfunc: 'eff_s * s / np.sqrt(eff_s*s + eff_b*b)'
    #lossfunc: 'eff_s * s / (eff_s*s + eff_b*b)'

    # Approximate median significance (AMS)
    lossfunc: 'np.sqrt(2*((eff_s*s + eff_b*b + b_r)*np.log(1 + eff_s*s / (eff_b*b + b_r)) - eff_s*s))'
    
    lossfunc_var:              # Constants corresponding to 'lossfunc', expanded into memory via (exec)
      s: 20                    # Number of signal events expected
      b: 1000                  # Number of background events expected
      b_r: 10                  # Regularization constant
  
  variable: ['x_hlt_pms2',
             'x_hlt_invEInvP',
             'x_hlt_trkDEtaSeed',
             'x_hlt_trkDPhi',
             'x_hlt_trkChi2',
             'x_hlt_trkValidHits',
             'x_hlt_trkNrLayerIT']
  
  model_param:

    comp_0: '<'       # hint: for raytune, use "tune.choice(['<', '>'])"
    comp_1: '<'
    comp_2: '<'
    comp_3: '<'
    comp_4: '<'
    comp_5: '>='
    comp_6: '>='

    cut_0: 10000
    cut_1:   0.2
    cut_2:  0.01
    cut_3:   0.2
    cut_4:    40
    cut_5:     5
    cut_6:     2

    logic_0: '&&' # &&, ||
  
  # Using yaml multiline suntax without last linebreak syntax with >-
  # https://stackoverflow.com/questions/3790454/how-do-i-break-a-string-in-yaml-over-multiple-lines
  cutstring: >-
    ({variable[0]} {comp_0} {cut_0}) {logic_0}
    ({variable[1]} {comp_1} {cut_1}) {logic_0}
    ({variable[2]} {comp_2} {cut_2}) {logic_0}
    ({variable[3]} {comp_3} {cut_3}) {logic_0}
    ({variable[4]} {comp_4} {cut_4}) {logic_0}
    ({variable[5]} {comp_5} {cut_5}) {logic_0}
    ({variable[6]} {comp_6} {cut_6})

cut0:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_pms2 x (-1)'
  variable: 'x_hlt_pms2'
  sign: -1
  transform: null

cut1:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_invEInvP x (-1)'
  variable: 'x_hlt_invEInvP'
  sign: -1
  transform: 'np.tanh'

cut2:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkDEtaSeed x (-1)'
  variable: 'x_hlt_trkDEtaSeed'
  sign: -1
  transform: 'np.tanh'

cut3:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkDPhi x (-1)'
  variable: 'x_hlt_trkDPhi'
  sign: -1
  transform: 'np.tanh'

cut4:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkChi2 x (-1)'
  variable: 'x_hlt_trkChi2'
  sign: -1
  transform: 'np.tanh'

cut5:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkValidHits'
  variable: 'x_hlt_trkValidHits'
  sign: 1
  transform: null

cut6:
  train:    'cut'
  predict:  'cut'
  label:    'hlt_trkNrLayerIT'
  variable: 'x_hlt_trkNrLayerIT'
  sign: 1
  transform: null


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  xgb_setup_0

  # booster parameters
  model_param:
    num_boost_round: 56       # number of epochs (equal to the number of trees!)
  
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'       # 'auto', 'cpu', 'cuda:0'
    
    learning_rate: 0.1
    gamma: 1.67
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 4.8          # L2 regularization
    reg_alpha: 0.05          # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'          # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']              # for evaluation, 'logloss', 'mlogloss'
  
  plot_trees: false
  tensorboard: true

  # Read/Write of epochs
  evalmode: 1                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb1:
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB_tiny'
  raytune: 'xgb_setup_1'
  
  # booster parameters
  model_param:
    num_boost_round: 2      # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'       # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'     # 'auto', 'cpu', 'cuda'

    learning_rate: 0.1
    gamma: 0.63
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.8
    colsample_bylevel: 0.9
    colsample_bynode:  0.95
    
    reg_lambda: 1.37       # L2 regularization
    reg_alpha: 0.35        # L1 regularization
    
    # learning task parameters
    objective:  'binary:logistic' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']     # for evaluation, 'mlogloss'
  
  plot_trees: true
  tensorboard: true

  # Read/Write of epochs
  evalmode: 1                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch


# Logistic Regression (convex model = global optimum guarantee)
lgr0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'LGR'
  raytune:  null

  # Model param
  conv_type: 'lgr'
  model_param:
    null

  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 0.00001       # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1
  
  device: 'auto'                # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  tensorboard: true

  # Read/Write of epochs
  evalmode: 1                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                  # -1 is the last saved epoch
  
  eval_batch_size: 4096

# -----------------------------------------------------------------------------
# Remember to use 'zscore-weighted' (or 'zscore') typically with Neural Networks,
# however, performance with BDTs may be better without.
# -----------------------------------------------------------------------------

# Deep MLP
dmlp0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP'
  raytune:  null

  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [64, 64]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
    
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy' # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                  # focal_entropy exponent
    temperature: 1            # logit norm temperature
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 50
    batch_size: 196
    lr: 1.0e-3
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    type: 'step'               # 'cos', 'warm-cos', 'exp', 'step', 'constant' (deeptools.set_scheduler)
    step_size: 200
    gamma: 0.1

  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4
  tensorboard: true
  
  # Read/Write of epochs
  evalmode: 1                  # Every n-th epoch (integer), evaluation done also for every n-th
  readmode: -1                 # -1 is the last saved epoch

  eval_batch_size: 4096
