# Low-pT Electron ID tune0.yml
#
# General comments:
#
# - Keep the minibatch size small enough, otherwise weak convergence to optima with some models
# - Other imputation methods than 'constant' not natively compatible with re-weighting (-> biased)

rootname: 'brem'
rngseed: 123456                       # Fixed seed for training data mixing
num_cpus: 0                           # 0 for automatic
inputvars: 'mvavars'                  # Main file input description python file

# ----------------------------------------------------
mva_param: &MVA_INPUT_PARAM
  use_conditional: false              # Conditional (theory parametric) input
  primary_classes: [0,1]              # Primary class IDs in MVA (train, ROC etc.)
  signal_class: 1                     # Signal class ID
  #DA_class:    -2                    # Domain Adaptation class
  
  inputvar_scalar: 'CMSSW_MVA_SCALAR_VARS' # Input variables, implemented under mvavars.py
  inputvar_image: null                # Image variables, set 'CMSSW_MVA_IMAGE_VARS'

  varnorm: null                       # Variable normalization: 'zscore', 'zscore-weighted', 'madscore', null
  
  frac: [0.6, 0.1, 0.3]               # Train vs validate/test split fraction
  
  # Imputation
  imputation_param:
    active: true                      # True / False
    var: 'CMSSW_MVA_SCALAR_VARS'      # Array of variables to be imputated
    algorithm: 'constant'             # Algorithm type: 'constant', iterative' (vector), knn' (vector), 'mean' (scalar), 'median' (scalar)
    fill_value: -10                   # For constant imputation
    knn_k: 8                          # Number of nearest neighbours considered
    values: [-999, -666, -10]         # Special values which indicate the need for imputation, if null, then only Inf/Nan are imputed
  
# ----------------------------------------------------
genesis_runmode:

  maxevents:  null
  inputmap:   null
  tree_name:  'ntuplizer/tree'
  
  targetfunc: 'target_e'              # Training target,    implemented under mctargets.py
  filterfunc: 'filter_standard'       # Training filtering, implemented under mcfilter.py
  cutfunc:    'cut_standard'          # Basic cuts,         implemented under cuts.py

  xcorr_flow: True                    # Full N-point correlations computed between cuts
  pickle_size: 1000000                # Number of entries (events) per pickle file

# ----------------------------------------------------
train_runmode:

  <<: *MVA_INPUT_PARAM

  maxevents: null
  modeltag:  null

  tech: &TECH
    concat_max_pickle: 32           # (technical) [Recursive] concatenation max size, adjust this if encounter problems e.g. with awkward ("127 limit")
  
  ## Reweighting setup
  reweight: true
  reweight_mode: 'write'              # 'write', 'load'
  reweight_file: null                 # null for automatic, or string for specific
  
  reweight_param: &REWEIGHT_PARAM

    equal_frac: true           # Equalize integrated class fractions
    differential: true         # Differential reweighting
    reference_class: 0         # Reference (target) class: 0 = (background), 1 = (signal), 2 = (another class) ..., 

    # Note about asymmetry: Keep the reference class as 0 == background, because signal
    # MC has much longer tail over pt. Thus, it is not possible to re-weight background
    # to signal very easily, but it is possible to re-weight signal to background.
    # This is seen by inspecting the re-weighted distributions with small and large statistics.
    
    # Differential reweighting param
    diff_param:

      maxevents: 1000000              # Maximum number of events for the PDF construction
      renorm_weight_to_count: True    # Renormalize sum(weights) == sum(counts) per class

      var:  ['gsf_pt', 'gsf_eta']
      type: '2D'                    # 'AIRW', 'pseudo-ND', '2D', '1D'
      
      hist_param:

        pseudo_type: 'product'                    # 'product', 'geometric_mean'

        bins:  [[0.1, 20.0, 40], [-2.5, 2.5, 40]] # Make sure the bounds cover the phase space
        binmode:  ['log10', 'linear']             # 'log10' or 'linear' or 'edges' (custom array) binning
        transform: [null, null]                   # 'log10', 'sqrt', 'square', null

        max_reg:      10.0        # Maximum weight cut-off regularization
      
      AIRW_param:
        active_model: xgb3Drw     # Under models.yml
        max_reg:      10.0        # Maximum weight cut-off regularization
        mode:         'LR'        # 'LR', 'inverse-LR', 'DeepEfficiency', 'direct'
  
  ## Outlier protection in the training
  outlier_param:
    algo: 'truncate'   # algorithm: 'truncate', null
    qmin: 0.01         # in [0,100] 
    qmax: 99.9         # in [0,100]

    truncate_weights: True # Truncate outlier event weights
    process_validate: True # Protect also validation sample

  # ** Activate models here **
  # Give all models some unique identifier and label
  models:  !include configs/brem/models.yml
  active_models: &ACTIVE_MODELS

#    - xgb0
    - xgb1
    - cut0
    - cut1
  
  raytune: !include configs/brem/raytune.yml

# ----------------------------------------------------
eval_runmode:
  
  <<: *MVA_INPUT_PARAM
  
  maxevents: null
  modeltag:  null
  
  tech: *TECH
  
  reweight: false
  reweight_mode: 'load'  # 'write', 'load'
  reweight_file: null    # null for automatic, or string for specific
  
  reweight_param: *REWEIGHT_PARAM

  models:  !include configs/brem/models.yml
  active_models: *ACTIVE_MODELS

# ----------------------------------------------------
plot_param: !include configs/brem/plots.yml
