## MVA models

# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb0:
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'XGB'
  raytune:  xgb_trial_0
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # booster parameters
  model_param:
    num_boost_round: 200       # number of epochs (equal to the number of trees!)
    
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'       # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 1.67
    max_depth: 10
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 1.0            # L2 regularization
    reg_alpha: 0.05            # L1 regularization
    
    # learning task parameters
    objective: 'custom:binary_cross_entropy' # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['custom']                  # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0,1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  plot_trees: False
  
  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch


# Deep MLP
dmlp0:
  train:   'torch_generic'
  predict: 'torch_vector'
  label:   'DMLP'
  raytune:  null
  
  # ** Custom set of variables **
  #include_MVA_vars: ['.*']
  #exclude_MVA_vars: ['.*']
  
  # Model
  conv_type: 'dmlp'
  model_param:
    mlp_dim: [32, 32]     # hidden layer dimensions
    activation: 'relu'
    batch_norm: False
    dropout: 0.01
  
  # Optimization
  opt_param:  
    lossfunc: 'cross_entropy'  # cross_entropy, focal_entropy, logit_norm_cross_entropy
    gamma: 2                   # focal_entropy exponent
    temperature: 1             # logit norm temperature
    
    optimizer: 'AdamW'
    clip_norm: 1.0
    
    epochs: 150
    batch_size: 256
    lr: 3.0e-4
    weight_decay: 0.00001      # L2-regularization

  # Scheduler
  scheduler_param:
    step_size: 250             # Number of epochs for drop
    gamma: 0.1
  
  device: 'auto'               # alternative 'cpu:0', 'cuda:0'
  num_workers: 4

  # Read/Write of epochs
  savemode: 'all'              # 'all', 'latest'
  readmode: -1                 # -1 is the last saved epoch

