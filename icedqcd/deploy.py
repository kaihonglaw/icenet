# Deploy MVA-model to data (or MC) files (work in progress, not all models supported ...)
#
# m.mieskolainen@imperial.ac.uk, 2023

import os
import numpy as np
import awkward as ak
import pickle
import uproot
import logging
import copy
import socket
import gc

from datetime import datetime
from termcolor import colored, cprint


# GLOBALS
from configs.dqcd.mvavars import *

from icedqcd import common
from icenet.tools import iceroot, io, aux, process
from icenet.deep import predict
from tqdm import tqdm


def generate_cartesian_param(ids):
    """
    Generate cartesian N-dim array for the theory model parameter conditional sampling

    Note. Keep the order m, ctau, xiO, xiL
    """

    values    = {'m':    np.round(np.array([2.0, 3.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0]), 1),
                 'ctau': np.round(np.array([1.0, 5.0, 10, 25, 50, 75, 100, 250, 500]), 1),
                 'xiO':  np.round(np.array([1.0]), 1),
                 'xiL':  np.round(np.array([1.0]), 1)}

    CAX       = aux.cartesian_product(*[values['m'], values['ctau'], values['xiO'], values['xiL']])

    pindex    = np.zeros(4, dtype=int)
    pindex[0] = ids.index('MODEL_m')
    pindex[1] = ids.index('MODEL_ctau')
    pindex[2] = ids.index('MODEL_xiO')
    pindex[3] = ids.index('MODEL_xiL')
    
    return CAX, pindex

def f2s(value):
    """
    Convert floating point "1.5" to "1p5"
    """
    return str(np.round(value,1)).replace('.', 'p')

def zscore_normalization(X, args):
    """
    Z-score normalization
    """
    if   args['varnorm'] == 'zscore':                            
        print(__name__ + f'.process_data: Z-score normalizing variables ...')
        X_mu, X_std = pickle.load(open(args["modeldir"] + '/zscore.pkl', 'rb'))

        print(__name__ + f'.process_data: X.shape = {X.shape} | X_mu.shape = {X_mu.shape} | X_std.shape = {X_std.shape}')
        X = io.apply_zscore(X, X_mu, X_std)

    return X

def process_data(args):

    print(args)

    ## ** Special YAML loader here **
    from libs.ccorp.ruamel.yaml.include import YAML
    yaml = YAML()
    yaml.allow_duplicate_keys = True
    cwd = os.getcwd()

    with open(f'{cwd}/configs/dqcd/{args["inputmap"]}', 'r') as f:
        try:
            inputmap = yaml.load(f)
            if 'includes' in inputmap:
                del inputmap['includes'] # ! crucial

        except yaml.YAMLError as exc:
            print(f'yaml.YAMLError: {exc}')
            exit()

    root_path = args['root_files']
    if type(root_path) is list:
        root_path = root_path[0] # Remove [] list

    # ------------------
    # Phase 1: Read ROOT-file to awkward-format

    VARS  = []
    VARS += TRIGGER_VARS
    VARS += MVA_SCALAR_VARS
    VARS += MVA_JAGGED_VARS
    VARS += MVA_PF_VARS

    basepath = aux.makedir(f"{cwd}/output/dqcd/deploy/modeltag__{args['modeltag']}")

    nodestr  = (f"inputmap__{io.safetxt(args['inputmap'])}--grid_id__{args['grid_id']}--hostname__{socket.gethostname()}--time__{datetime.now()}").replace(' ', '')
    logging.basicConfig(filename=f'{basepath}/deploy--{nodestr}.log', encoding='utf-8',
        level=logging.DEBUG, format='%(asctime)s | %(message)s', datefmt='%d/%m/%Y %H:%M:%S')

    # Save to log-file
    logging.debug(__name__ + f'.process_data: {nodestr}')
    logging.debug('')
    logging.debug(f'{inputmap}')
    
    for key in inputmap.keys():

        print(__name__ + f'.process_data: Processing "{key}"')

        # Write to log-file
        logging.debug('-------------------------------------------------------')
        logging.debug(f'Process: {key}')

        # Get all files
        datasets  = inputmap[key]['path'] + '/' + inputmap[key]['files']
        rootfiles = io.glob_expand_files(datasets=datasets, datapath=root_path)

        # Loop over the files
        total_num_events = 0

        # ----------------------------------
        # GRID (batch) processing
        all_file_id = aux.split_start_end(range(len(rootfiles)), args['grid_nodes'])

        try:
            file_id = all_file_id[args['grid_id']]
            print(__name__ + f".deploy: file_id = {file_id} (grid_id = {args['grid_id']})")
            
        except Exception as e:
            logging.debug('Subfolder already saturated -- no processing under this grid PC node -- continue')
            continue # This subfolder is already saturated on the grid processing, i.e. grid_id > len(all_file_id)
        # ----------------------------------
        
        for k in range(file_id[0], file_id[-1]):

            filename = rootfiles[k]
            param = {
                'rootfile'    : filename,
                'tree'        : 'Events',
                'entry_start' : None,
                'entry_stop'  : None,
                'maxevents'   : None,
                'ids'         : LOAD_VARS,
                'library'     : 'ak'
            }
            
            try:
                X_nocut, ids_nocut = iceroot.load_tree(**param)

                # Write to log-file
                logging.debug(f'{filename} | Number of events: {len(X_nocut)}')
                total_num_events += len(X_nocut)

            except Exception as e:
                cprint(__name__ + f'.process_data: A fatal error in iceroot.load_tree with a file "{filename}": ' + str(e), 'red')
                
                # Write to log-file
                logging.debug(f'{filename} | A fatal error in iceroot.load_tree: ' + str(e))
                continue
            
            # -------------------------------------------------
            # Add conditional (theory param) variables
            model_param = {'m': 0.0, 'ctau': 0.0, 'xiO': 0.0, 'xiL': 0.0}

            if args['use_conditional']:

                print(__name__ + f'.process_data: Initializing conditional theory (model) parameters')
                for var in model_param.keys():
                    # Create new 'record' (column) to ak-array
                    col_name    = f'MODEL_{var}'
                    X_nocut[col_name] = model_param[var]

                ids_nocut = ak.fields(X_nocut)

            # ------------------
            # Phase 2: Apply pre-selections to get an event mask

            mask = common.process_root(X=X_nocut, args=args, return_mask=True)
            
            if np.sum(mask) == 0:
                X        = X_nocut
                logging.debug("No events left after pre-cuts -- scores will be -1 for all events")
                OK_event = False
            else:
                X        = X_nocut[mask]
                OK_event = True
            
            # ------------------
            # Phase 3: Convert to icenet dataformat

            try:
                data = common.splitfactor(x=X, y=None, w=None, ids=ids_nocut, args=args, skip_graph=True)
            except Exception as e:
                
                # Something went wrong at OS level (e.g. memory), write to log-file and exit with error
                txt = f'{filename} | A fatal error in common.splitfactor -- os._exit(os.EX_OSERR): ' + str(e)
                logging.debug(txt)
                print(txt)
                os._exit(os.EX_OSERR)
            
            # ------------------
            # ** Save memory **
            del X
            del X_nocut
            gc.collect()
            # ------------------
            
            # ------------------
            # Phase 4: Apply MVA-models
            
            ALL_scores = {}

            for i in range(len(args['active_models'])):
                
                gc.collect()
            
                ID    = args['active_models'][i]
                param = args['models'][ID]
                
                if param['predict'] == 'xgb':

                    print(f'Evaluating MVA-model "{ID}" \n')

                    # Impute data
                    if args['imputation_param']['active']:
                        imputer = pickle.load(open(args["modeldir"] + f'/imputer.pkl', 'rb'))
                        data['data'], _  = process.impute_datasets(data=data['data'], features=None, args=args['imputation_param'], imputer=imputer)

                    ## Apply the input variable set reductor
                    X,ids = aux.red(X=data['data'].x, ids=data['data'].ids, param=param)

                    '''
                    print("ids = ", ids)
                    print("len = ", len(ids))
                    import sys
                    sys.exit()
                    '''

                    ## Get the MVA-model
                    func_predict, model = get_predictor(args=args, param=param, feature_names=ids)
                    
                    '''
                    model.save_model("/vols/cms/khl216/model_ul_saved.model")
                    import sys
                    sys.exit()
                    '''

                    ## ** Conditional model **
                    if args['use_conditional']:

                        ## Get conditional parameters
                        CAX,pindex = generate_cartesian_param(ids=ids)

                        ## Run the MVA-model on all the theory model points in CAX array
                        for z in tqdm(range(len(CAX))):
                            
                            gc.collect()

                            # Set the new conditional model parameters to X
                            nval     = CAX[z,:]
                            ID_label = f'{ID}__m_{f2s(nval[0])}_ctau_{f2s(nval[1])}_xiO_{f2s(nval[2])}_xiL_{f2s(nval[3])}'

                            if OK_event:
                                XX            = copy.deepcopy(X)
                                XX[:, pindex] = nval  # Set new values

                                # Variable normalization
                                XX = zscore_normalization(X=XX, args=args)

                                # Predict
                                pred = func_predict(XX)
                                pred = aux.unmask(x=pred, mask=mask, default_value=-1)
                            else:
                                pred = (-1) * np.ones(len(mask)) # all -1

                            # Save
                            ALL_scores[io.rootsafe(ID_label)] = pred

                    # ** Unconditional model **
                    else:

                        if OK_event:

                            # Variable normalization
                            XX = copy.deepcopy(X)
                            XX = zscore_normalization(X=XX, args=args)

                            # Predict
                            pred = func_predict(XX)
                            pred = aux.unmask(x=pred, mask=mask, default_value=-1)
                                               
                        else:
                            pred = (-1) * np.ones(len(mask)) # all -1

                        # Save
                        ALL_scores[io.rootsafe(ID)] = pred

                else:
                    
                    # if param['predict'] == 'torch_graph': # Turned off for now
                    #   scores = func_predict(data['graph'])

                    # Write to log-file
                    logging.debug(f'Did not evaluate model (unsupported deployment): {ID}')
                    continue
            
            # ------------------
            # Phase 5: Write MVA-scores out

            aux.makedir(basepath + '/' + filename.rsplit('/', 1)[0]) # Create dir
            outputfile = basepath + '/' + filename.replace('.root', '-icenet.root')
            
            print(__name__ + f'.process_data: Saving root output to "{outputfile}"')

            with uproot.recreate(outputfile, compression=uproot.ZLIB(4)) as rfile:
                rfile[f"Events"] = ALL_scores
            
            gc.collect() #! Garbage collection
        
        # Write to log-file
        logging.debug(f'Total number of events: {total_num_events}')

def get_predictor(args, param, feature_names=None):    

    model = None

    if   param['predict'] == 'xgb':
        func_predict, model = predict.pred_xgb(args=args, param=param, feature_names=feature_names, return_model=True)

    elif param['predict'] == 'xgb_logistic':
        func_predict, model = predict.pred_xgb_logistic(args=args, param=param, feature_names=feature_names, return_model=True)

    elif param['predict'] == 'torch_vector':
        func_predict = predict.pred_torch_generic(args=args, param=param)

    elif param['predict'] == 'torch_scalar':
        func_predict = predict.pred_torch_scalar(args=args, param=param)

    elif param['predict'] == 'torch_flow':
        func_predict = predict.pred_flow(args=args, param=param, n_dims=X_ptr.shape[1])

    elif param['predict'] == 'torch_graph':
        func_predict = predict.pred_torch_graph(args=args, param=param)

    elif param['predict'] == 'graph_xgb':
        func_predict = predict.pred_graph_xgb(args=args, param=param)
    
    elif param['predict'] == 'torch_deps':
        func_predict = predict.pred_torch_generic(args=args, param=param)
    
    elif param['predict'] == 'torch_image':
        func_predict = predict.pred_torch_generic(args=args, param=param)
        
    elif param['predict'] == 'torch_image_vector':
        func_predict = predict.pred_torch_generic(args=args, param=param)

    elif param['predict'] == 'flr':
        func_predict = predict.pred_flr(args=args, param=param)
    
    elif param['predict'] == 'exp':
        func_predict = predict.pred_exp(args=args, param=param)
    
    elif param['predict'] == 'cut':
        func_predict = predict.pred_cut(args=args, param=param)
    
    elif param['predict'] == 'cutset':
        func_predict = predict.pred_cutset(args=args, param=param)
    
    else:
        raise Exception(__name__ + f'.Unknown param["predict"] = {param["predict"]} for ID = {ID}')

    return func_predict, model

"""
def apply_models(data=None, args=None):
    #
    #Evaluate ML/AI models.
    #
    #Args:
    #    Different datatype objects (see the code)
    #

    try:
        ### Tensor variable normalization
        if data['data_tensor'] is not None and (args['varnorm_tensor'] == 'zscore'):

            print('\nZ-score normalizing tensor variables ...')
            X_mu_tensor, X_std_tensor = pickle.load(open(args["modeldir"] + '/zscore_tensor.pkl', 'rb'))
            X_2D = io.apply_zscore_tensor(X_2D, X_mu_tensor, X_std_tensor)
        
        ### Variable normalization
        if   args['varnorm'] == 'zscore':

            print('\nZ-score normalizing variables ...')
            X_mu, X_std = pickle.load(open(args["modeldir"] + '/zscore.pkl', 'rb'))
            X = io.apply_zscore(X, X_mu, X_std)

        elif args['varnorm'] == 'madscore':

            print('\nMAD-score normalizing variables ...')
            X_m, X_mad = pickle.load(open(args["modeldir"] + '/madscore.pkl', 'rb'))
            X = io.apply_madscore(X, X_m, X_mad)

    except:
        cprint('\n' + __name__ + f' WARNING: {sys.exc_info()[0]} in normalization. Continue without! \n', 'red')
    
    # --------------------------------------------------------------------
    # For pytorch based
    if X is not None:
        X_ptr      = torch.from_numpy(X).type(torch.FloatTensor)

    if X_2D is not None:
        X_2D_ptr   = torch.from_numpy(X_2D).type(torch.FloatTensor)
        
    if X_deps is not None:
        X_deps_ptr = torch.from_numpy(X_deps).type(torch.FloatTensor)
"""
